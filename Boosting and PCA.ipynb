{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Boosting and PCA\n",
    "\n",
    "\n",
    "This assignment is due on Moodle by **11:59pm on Friday November 22**. \n",
    "Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.\n",
    "Your solutions to computational questions should include any specified Python code and results \n",
    "as well as written commentary on your conclusions.\n",
    "Remember that you are encouraged to discuss the problems with your instructors and classmates, \n",
    "but **you must write all code and solutions on your own**. For a refresher on the course **Collaboration Policy** click [here](https://github.com/BoulderDS/CSCI5622-Machine-Learning/blob/master/info/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda (Version: 2019.07) with Python 3.7. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc.\n",
    "- **Unzip the files in data folder**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Andrew Gerlach ange6809"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[40 points] Problem 1 - Principal Component Analysis\n",
    "---\n",
    "\n",
    "In this problem you'll be implementing Dimensionality reduction using Principal Component Analysis technique. \n",
    "\n",
    "The gist of PCA Algorithm to compute principal components is follows:\n",
    "- Calculate the covariance matrix X of data points.\n",
    "- Calculate eigenvectors and corresponding eigenvalues.\n",
    "- Sort the eigenvectors according to their eigenvalues in decreasing order.\n",
    "- Choose first k eigenvectors which satisfies target explained variance.\n",
    "- Transform the original n dimensional data points into k dimensions.\n",
    "\n",
    "The skeleton for the `PCA` class is below. Scroll down to find more information about your tasks as well as unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "794cc99f3b7d4279d779dd191b40e827",
     "grade": false,
     "grade_id": "cell-34ffc5741eeb12ad",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, target_explained_variance=None):\n",
    "        \"\"\"\n",
    "        explained_variance: float, the target level of explained variance\n",
    "        \"\"\"\n",
    "        self.target_explained_variance = target_explained_variance\n",
    "        self.feature_size = -1\n",
    "\n",
    "    def standardize(self, X):\n",
    "        \"\"\"\n",
    "        standardize features using standard scaler\n",
    "        :param m X n: features data\n",
    "        :return: standardized features\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        X_T = X.T\n",
    "        X_std = np.zeros(shape = X_T.shape)\n",
    "        for i in range(len(X_T)):\n",
    "            curr_mean = np.mean(X_T[i])\n",
    "            curr_std = np.std(X_T[i])\n",
    "            std_row = (X_T[i] - curr_mean) / curr_std\n",
    "            X_std[i] = std_row\n",
    "        return X_std.T\n",
    "\n",
    "    def compute_mean_vector(self, X_std):\n",
    "        \"\"\"\n",
    "        compute mean vector\n",
    "        :param X_std: data\n",
    "        :return n X 1 matrix: mean vector\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        mean_vector = np.zeros(shape = (X_std.shape[1],))\n",
    "        \n",
    "        for i in range(X_std.shape[1]):\n",
    "            curr_sum = sum(X_std[:,i])\n",
    "            mean_vector[i] = curr_sum / len(X_std[:,i])\n",
    "            \n",
    "        return mean_vector\n",
    "             \n",
    "\n",
    "    def compute_cov(self, X_std, mean_vec):\n",
    "        \"\"\"\n",
    "        Covariance using mean, (don't use any numpy.cov)\n",
    "        :param X_std:\n",
    "        :param mean_vec:\n",
    "        :return n X n matrix:: covariance matrix\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        cov_matrix = np.zeros(shape = (len(mean_vec), len(mean_vec)))\n",
    "        temp = np.zeros(shape = X_std.shape)\n",
    "        temp = X_std - mean_vec\n",
    "        for i in range(temp.shape[1]):\n",
    "            cov_i = temp[:,i]\n",
    "            for j in range(temp.shape[1]):\n",
    "                cov_j = temp[:,j]\n",
    "                cov_i_j = np.dot(cov_i, cov_j) / (temp.shape[0] - 1)\n",
    "                cov_matrix[i][j] = cov_i_j\n",
    "                \n",
    "        return cov_matrix\n",
    "            \n",
    "\n",
    "    def compute_eigen_vector(self, cov_mat):\n",
    "        \"\"\"\n",
    "        Eigenvector and eigen values using numpy\n",
    "        :param cov_mat:\n",
    "        :return: (eigen_vector,eigen_values)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return np.linalg.eig(cov_mat)\n",
    "\n",
    "    def compute_explained_variance(self, eigen_vals):\n",
    "        \"\"\"\n",
    "        sort eigen values and compute explained variance.\n",
    "        explained variance informs the amount of information (variance)\n",
    "        can be attributed to each of  the principal components.\n",
    "        :param eigen_vals:\n",
    "        :return: explained variance.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        sum_of_eigen_vals = np.sum(eigen_vals)\n",
    "        \n",
    "        #Fill the list with the variances in decreasing order\n",
    "        explained_variance = [ev/sum_of_eigen_vals for ev in sorted(eigen_vals, reverse = True)]\n",
    "        \n",
    "        return explained_variance\n",
    "        \n",
    "\n",
    "    def cumulative_sum(self, var_exp):\n",
    "        \"\"\"\n",
    "        return cumulative sum of explained variance.\n",
    "        :param var_exp: explained variance\n",
    "        :return: cumulative explained variance\n",
    "        \"\"\"\n",
    "        return np.cumsum(var_exp)\n",
    "\n",
    "    def compute_weight_matrix(self, eig_pairs, cum_var_exp):\n",
    "        \"\"\"\n",
    "        compute weight matrix of top principal components conditioned on target\n",
    "        explained variance.\n",
    "        (Hint : use cumulative explained variance and target_explained_variance to find\n",
    "        top components)\n",
    "        \n",
    "        :param eig_pairs: list of tuples containing eigenvector and eigen\n",
    "        values\n",
    "        :param cum_var_exp: cumulative explained variance by features\n",
    "        :return: weight matrix\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        weight_matrix = []\n",
    "        #sort the eigen_pairs list in descending order\n",
    "        eig_pairs_sorted = sorted(eig_pairs, key = lambda x : x[1], reverse = True)\n",
    "        for i in range(len(cum_var_exp)):\n",
    "            if cum_var_exp[i] <= self.target_explained_variance:\n",
    "                weight_matrix.append(eig_pairs_sorted[i][0]) \n",
    "        self.feature_size = len(weight_matrix)\n",
    "        return np.transpose(weight_matrix)\n",
    "    \n",
    "    def transform_data(self, X_std, matrix_w):\n",
    "        \"\"\"\n",
    "        transform data to subspace using weight matrix\n",
    "        :param X_std: standardized data\n",
    "        :param matrix_w: weight matrix\n",
    "        :return: data in the subspace\n",
    "        \"\"\"\n",
    "        return X_std.dot(matrix_w)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        entry point to the transform data to k dimensions\n",
    "        standardize and compute weight matrix to transform data.\n",
    "        :param   m X n dimension: train samples\n",
    "        :return  m X k dimension: subspace data.\n",
    "        \"\"\"\n",
    "        self.feature_size = X.shape[1]\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        X_std = self.standardize(X)\n",
    "        \n",
    "        mean_vector = self.compute_mean_vector(X_std)\n",
    "        cov_matrix = self.compute_cov(X_std, mean_vector)\n",
    "        eigen_vals,eigen_vecs = self.compute_eigen_vector(cov_matrix)\n",
    "        \n",
    "        eigen_vecs = eigen_vecs.T\n",
    "        \n",
    "        exp_variance = self.compute_explained_variance(eigen_vals)\n",
    "\n",
    "        cum_sum = self.cumulative_sum(exp_variance)\n",
    "\n",
    "        eig_pairs = []\n",
    "        for i in range(len(eigen_vals)):\n",
    "            eig_pairs.append((eigen_vecs[i], eigen_vals[i]))\n",
    "            \n",
    "        matrix_w = self.compute_weight_matrix(eig_pairs, cum_sum)\n",
    "\n",
    "        return self.transform_data(X_std = X_std, matrix_w = matrix_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf42ca03885d2b460e78bf541d2351e4",
     "grade": false,
     "grade_id": "cell-f115fedc1c3aac43",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part 1 [20 points]:**  Your task involves implementing helper functions to compute mean, \n",
    "covariance, eigenvector and weights.\n",
    "\n",
    "Complete `fit` to use all helper functions to find reduced dimension data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11ba3527363fb8065febc0871d978739",
     "grade": true,
     "grade_id": "cell-1395ce0d605dc74a",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from tests import tests\n",
    "tests.run_test_suite(\"prob 1\", PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 [5 points]:**   Run PCA on fashion mnist dataset to reduce the dimension of the data.\n",
    "\n",
    "fashion mnist data consists of samples with 784 dimensions.\n",
    "\n",
    "Report the reduced dimension $k$ for target explained variance of **0.99**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open('./data/mnist/train_images.pkl','rb'))\n",
    "y_train = pickle.load(open('./data/mnist/train_image_labels.pkl','rb'))\n",
    "X_train = X_train[:15000]\n",
    "y_train = y_train[:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b15912907fa9d00f82d5c8e82581047b",
     "grade": true,
     "grade_id": "cell-4f2deef7b2ad8f4c",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "pca = PCA(target_explained_variance = 0.99)\n",
    "X_train_updated = pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data features went from {0} to {1} \".format(X_train.shape[1] , pca.feature_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3 [5 points]:** Run scikit-learn SVM Classifier (refer previous homework) on the reduced dimension data with approrpiate kernel and C.\n",
    "\n",
    "Report the accuracy on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_t, X_test, y_t, y_test = train_test_split(X_train_updated, y_train, test_size=0.2, random_state=5622)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1e3c0aee9c27884fa3370d9620b8d50",
     "grade": true,
     "grade_id": "cell-55ac4663a6d818e1",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svm = SVC(degree = 3, gamma = 0.1, kernel = 'poly')\n",
    "svm.fit(X_t, y_t)\n",
    "print(\"Score on test data: {}\".format(svm.score(X_test, y_test)))\n",
    "# print(svm.predict(X_test))\n",
    "# print(svm.score(X_test, y_test))\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# svc = make_pipeline(SVC())\n",
    "\n",
    "# parameters = [{'svc__kernel': ['linear'], 'svc__C': [0.01, 1.0, 10.0]}, \n",
    "#  {'svc__kernel' : ['poly'], 'svc__degree' : [2, 3], 'svc__gamma' : [0.1, 0.5, 1.0] },\n",
    "#  {'svc__kernel': ['rbf'], 'svc__gamma': [0.1, 0.5, 1.0]}]\n",
    "\n",
    "# grid_svc = GridSearchCV(svc, param_grid = parameters, scoring = 'accuracy')\n",
    "\n",
    "# find_best = grid_svc.fit(X_t, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 [10 points]:** Repeat the same experiment for different values of target explained variance between: **[0.8-1.0]** with increments of $0.04$, provide the reduced dimension size for each, and then:\n",
    "\n",
    "- Plot the graph of accuracy vs target explained variance.\n",
    "- Plot the graph of the number of components vs target explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0a59b38a94aa997b5a62397ce22c6da",
     "grade": true,
     "grade_id": "cell-d1c639ba61115892",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X_train = pickle.load(open('./data/mnist/train_images.pkl','rb'))\n",
    "y_train = pickle.load(open('./data/mnist/train_image_labels.pkl','rb'))\n",
    "X_train = X_train[:15000]\n",
    "y_train = y_train[:15000]\n",
    "\n",
    "target_explained_variances = []\n",
    "numbers_of_components = []\n",
    "accuracies = []\n",
    "\n",
    "for target_variance in np.arange(0.8, 1.01, 0.04):\n",
    "    # YOUR CODE HERE\n",
    "    target_explained_variances.append(target_variance)\n",
    "    curr_pca = PCA(target_explained_variance = target_variance)\n",
    "    X_train_updated = curr_pca.fit(X_train)\n",
    "    numbers_of_components.append(X_train_updated.shape[1])\n",
    "    X_t, X_test, y_t, y_test = train_test_split(X_train_updated, y_train, test_size=0.2, random_state=5622)\n",
    "    svc = SVC(degree = 3, gamma = 0.1, kernel = 'poly')\n",
    "    svc.fit(X_t, y_t)\n",
    "    accuracies.append(svc.score(X_test,y_test))\n",
    "\n",
    "print(\"TEVs: \", target_explained_variances)\n",
    "print(\"# of Components: \", numbers_of_components)\n",
    "print(\"Accuracies: \", accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "384791d90ebfef520227aebf07572b05",
     "grade": true,
     "grade_id": "cell-b3d84112a8c56eb5",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make plots here\n",
    "# YOUR CODE HERE\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "ax.plot(target_explained_variances,  accuracies, color=\"black\")\n",
    "ax.set_xlabel(\"Target Explained Variances\", fontsize=16)\n",
    "ax.set_ylabel(\"Testing Accuracy\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots here\n",
    "# YOUR CODE HERE\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "ax.plot(target_explained_variances,  numbers_of_components, color=\"red\", label = \"Number of Components vs TEV\")\n",
    "ax.set_xlabel(\"Target Explained Variances\", fontsize=16)\n",
    "ax.set_ylabel(\"Number of Components\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss your observations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c7448f6358015c021d7af0aeb552751",
     "grade": true,
     "grade_id": "cell-ddf4e4516af2b2f2",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "As the Target Explained Variance increases the accuracy and the number of features increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[20 points] Problem 2 - Statistical PCA for non-zero mean random variables. \n",
    "---\n",
    "Let $x \\in R^D$ be a random vector. Let $\\mu_x$ = $E(x)$  $ \\in R^D$ and $\\sum_x = E(x − \\mu)(x − \\mu)$ be mean an covariance of x respectivley. \n",
    "\n",
    "We define *Principal components* of $x$ as $v_i$. Assuming that the eigenvalues of $\\sum_x$ are different from each other\n",
    "\n",
    "show that\n",
    "\n",
    "1) $v_1$ is the eigenvector of $\\sum_x$ corresponding to its largest eigenvalue.\n",
    "\n",
    "2) $v_2^T v_1$ = 0 and $v_2$ is the eigenvector of $\\sum_x$ corresponding to its second largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5964545d9ca55b8027aacfc1d212e96a",
     "grade": true,
     "grade_id": "cell-35f08a8505587ec1",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "1)\n",
    "The variance of each feature is given by $w^{T}$$C^{X}$$w$. Because we want he largest variance for the first principal component this can be found via:  $max_{w}$ $w^{T}$$C^{X}$$w$ s.t. $w^{T}$$w$ = 1. Using the Lagrangian $L(w,\\lambda) = w^{T}C^{X}w - \\lambda(w^{T}w - 1)$\n",
    "\n",
    "Set the gradient of L equal to zero to maximize w: $\\nabla_{w}L = 2C^{X}w - 2\\lambda w = 0$ \n",
    "\n",
    "From this we can see the way to maximize $w$ is to pick the largest value of lambda which will correspond to the first principal component, $v_1$.\n",
    "\n",
    "\n",
    "\n",
    "2)\n",
    "The same can be done to find the second principal component, $v_2$. The covariance matrix can be altered to remove $v_1$: $C^{X} = \\sum_{i = 2}^{D}\\lambda v_i v_i^{T}$ \n",
    "\n",
    "because $C^{X} = \\sum_{i = 2}^{D}\\lambda v_i v_i^{T} = \\frac{1}{m-1}(X^{T}X - (m-1)\\lambda v_i v_i^{T})$ ($v_1$ is removed).\n",
    "\n",
    "Then, the saem process as part 1 will find $v_2$ which will correspond to the second highest eigen value and be the second principal component.\n",
    "\n",
    "$v_{2}^{T} v_1 = 0$ because the covariance matrix is a symmetric matrix and in symmetric matrices the eigen vectors are orthogonal. The dot product between any 2 orthogonal vecotrs is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[40 points] Problem 3  - Decision Tree Ensembles: Bagging and Boosting\n",
    "---\n",
    "\n",
    "We are going to predict house price using decision tree ensembles.\n",
    "\n",
    "In this Regression problem, we compare Decision trees and it's ensembles - bagging and Boosting on House Price prediction dataset : https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "\n",
    "Make use of standard Regression API's of  Decision tree ensembles from sklearn to predict the house price. http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble\n",
    "\n",
    "**Part 1 [20 points]:**\n",
    "Complete the `ensemble_test` class to fit appropriate model recieved as parameter and store the test accuracy.\n",
    "Later we will also use `ensemble_test` class to plot score, metric and time taken to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8156ad724afbac97b3e99faf8fe9025",
     "grade": false,
     "grade_id": "cell-2f5b11c37751fad9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.metrics import explained_variance_score, precision_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pyp\n",
    "\n",
    "class EnsembleTest:\n",
    "    \"\"\"\n",
    "        Test multiple model performance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, type='regression'):\n",
    "        \"\"\"\n",
    "        initialize data and type of problem\n",
    "        :param X_train:\n",
    "        :param y_train:\n",
    "        :param X_test:\n",
    "        :param y_test:\n",
    "        :param type: regression or classification\n",
    "        \"\"\"\n",
    "        self.scores = {}\n",
    "        self.execution_time = {}\n",
    "        self.metric = {}\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.type = type\n",
    "        self.score_name = 'R^2 score' if self.type == 'regression' else 'Mean accuracy score'\n",
    "        self.metric_name = 'Explained variance' if self.type == 'regression' else 'Precision'\n",
    "\n",
    "    def fit_model(self, model, name):\n",
    "        \"\"\"\n",
    "        Fit the model on train data.\n",
    "        predict on test and store score and execution time for each fit.\n",
    "        :param model: model\n",
    "        :param name: name of model\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        start = time()\n",
    "        model_fit = model.fit(self.X_train, y_train)\n",
    "        self.scores[name] = model_fit.score(X_test, y_test)\n",
    "        predict = model_fit.predict(X_test)\n",
    "        finish = time()\n",
    "        self.execution_time[name] = finish - start\n",
    "        if self.type == 'regression':\n",
    "            self.metric[name] = explained_variance_score(predict, self.y_test)\n",
    "        elif self.type == 'classification':\n",
    "            self.metric[name] = precision_score(predict, y_test)\n",
    "\n",
    "    def print_result(self):\n",
    "        \"\"\"\n",
    "            print results for all models trained and tested.\n",
    "        \"\"\"\n",
    "        models_cross = pd.DataFrame({\n",
    "            'Model'         : list(self.metric.keys()),\n",
    "             self.score_name     : list(self.scores.values()),\n",
    "             self.metric_name    : list(self.metric.values()),\n",
    "            'Execution time': list(self.execution_time.values())})\n",
    "        print(models_cross.sort_values(by=self.score_name, ascending=False))\n",
    "\n",
    "    def plot_metric(self):\n",
    "        \"\"\"\n",
    "         plot each metric : time, metric score,scores\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        labels = list(self.metric.keys())\n",
    "        metrics = list(self.metric.values())\n",
    "        times = list(self.execution_time.values())\n",
    "        scores = list(self.scores.values())\n",
    "        \n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.25\n",
    "        fig, ax = pyp.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "        bar_1 = ax.bar(x - width, times, width, label='Execution Time (s)')\n",
    "        bar_2 = ax.bar(x, metrics, width, label=self.metric_name)\n",
    "        bar_3 = ax.bar(x + width, scores, width, label='Score')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.legend()\n",
    "        \n",
    "        for rect in bar_1:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(round(height,3)),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    " \n",
    "        for rect in bar_2:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(round(height,3)),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "            \n",
    "        for rect in bar_3:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(round(height,3)),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "        pyp.tight_layout()\n",
    "        pyp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pickle.load(open('./data/house_predictions/test_train.pkl','rb'))\n",
    "\n",
    "# create a handler for ensemble_test, use the created handler for fitting different models.\n",
    "ensemble_handler = EnsembleTest(X_train, y_train, X_test, y_test, type='regression')\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "decision = DecisionTreeRegressor()\n",
    "ensemble_handler.fit_model(decision,'decision tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 A [10 points]:**\n",
    "Complete the cells below to fit the dataset using `RandomForestRegressor` and `AdaBoostRegressor` with appropriate parameter. Use `n_estimators=1000` for both regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45d3a90b9307b4c71a691414ea6a5452",
     "grade": true,
     "grade_id": "cell-8a8faea9792d3de2",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# YOUR CODE HERE\n",
    "rfr = RandomForestRegressor(n_estimators = 1000)\n",
    "ensemble_handler.fit_model(rfr, 'random forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f667d8b508e624228f903db96928624e",
     "grade": true,
     "grade_id": "cell-77fc8d76bfc77a0f",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ada = AdaBoostRegressor()\n",
    "ensemble_handler.fit_model(ada, 'ada boost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 B [5 points]:** Report results and make plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8de42fcf797783cc942f9289ee054777",
     "grade": true,
     "grade_id": "cell-ecde0a3f23089aaf",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Report results here\n",
    "# YOUR CODE HERE\n",
    "print(ensemble_handler.print_result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbf7cb9c1a769ac7653efe66a138cd47",
     "grade": true,
     "grade_id": "cell-2fc10988e851c39e",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make plots here\n",
    "# YOUR CODE HERE\n",
    "ensemble_handler.plot_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 [15 points]:** This is an extension of HW4 problem on sentiment classification over reviews.\n",
    "\n",
    "Here we make use DecisionTree ensembles to **classify** review as positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews  = pd.read_csv('./data/reviews.csv')\n",
    "train, test = train_test_split(reviews, test_size=0.2, random_state=4622)\n",
    "X_train = train['reviews'].values\n",
    "X_test = test['reviews'].values\n",
    "y_train = train['sentiment']\n",
    "y_test = test['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 A [10 points]:** Perform the following: \n",
    "\n",
    "* Create pipeline for `RandomForestClassifier` and `AdaBoostClassifier` as shown for `DecisionTreeClassifier` below. Refer to: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble\n",
    "* Fit the reviews dataset on the above models and report the results. (tune parameters of classifier for optimal performance)\n",
    "* Use `n_estimators = 500` for both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer and create pipeline.\n",
    "def tokenize(text): \n",
    "    tknzr = TweetTokenizer()\n",
    "    return tknzr.tokenize(text)\n",
    "en_stopwords = set(stopwords.words(\"english\")) \n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words = en_stopwords,\n",
    "    min_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a handler for ensemble_test , use the created handler for fitting different models.\n",
    "ensemble_classifier_handler = EnsembleTest(X_train,y_train,X_test,y_test,type='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "pipeline_decision_tree = make_pipeline(vectorizer, DecisionTreeClassifier())\n",
    "ensemble_classifier_handler.fit_model(pipeline_decision_tree,'decision tree classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_classifier_handler.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36bfba82604c3aef47f990ead5202ab3",
     "grade": true,
     "grade_id": "cell-5ba823d25c616289",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# YOUR CODE HERE\n",
    "\n",
    "rfc = make_pipeline(vectorizer, RandomForestClassifier(n_estimators = 500, \n",
    "                                                       criterion = 'gini', \n",
    "                                                       max_depth = 30, \n",
    "                                                       min_samples_leaf = 1, \n",
    "                                                       min_samples_split = 5, \n",
    "                                                       oob_score = False))\n",
    "ensemble_classifier_handler.fit_model(rfc,'random forest classifier')\n",
    "ensemble_classifier_handler.print_result()\n",
    "\n",
    "\n",
    "#Find best parameters\n",
    "\n",
    "#print(rfc.get_params())\n",
    "# parameters = [\n",
    "# {'randomforestclassifier__max_depth' : [5, 8, 15, 25, 30],\n",
    "# 'randomforestclassifier__min_samples_split' : [2, 5, 10, 15, 30, 75, 100],\n",
    "# 'randomforestclassifier__min_samples_leaf' : [1, 2, 5, 10],\n",
    "# 'randomforestclassifier__oob_score' : [True, False], \n",
    "# 'randomforestclassifier__criterion': ['gini', 'entropy']}]\n",
    "\n",
    "# gridF = GridSearchCV(rfc, param_grid = parameters, verbose = 1)\n",
    "# bestF = gridF.fit(X_train, y_train)\n",
    "# print(bestF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1db4b5c2b0ca06fdc563f968143a202",
     "grade": true,
     "grade_id": "cell-0921e901522a923e",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# YOUR CODE HERE\n",
    "abc = make_pipeline(vectorizer, AdaBoostClassifier(n_estimators = 500, learning_rate = 0.25))\n",
    "ensemble_classifier_handler.fit_model(abc,'adaboost classifier')\n",
    "\n",
    "\n",
    "#Find best parameters\n",
    "\n",
    "# print(abc.get_params())\n",
    "#Trying different learning rates\n",
    "# parameters_ADA = [\n",
    "# {'adaboostclassifier__learning_rate': [0.01, 0.1, 0.25, 0.5, 0.75, 1.0]}]\n",
    "\n",
    "# gridADA = GridSearchCV(abc, param_grid = parameters_ADA, verbose = 1)\n",
    "# bestADA = gridADA.fit(X_train, y_train)\n",
    "# print(bestADA.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 B [5 points]:** Report results and make plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0b991c14cb2d605bac8f5fd8310b065",
     "grade": true,
     "grade_id": "cell-c19776c62148ed3f",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Report results here\n",
    "# YOUR CODE HERE\n",
    "ensemble_classifier_handler.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "305ac6686c2cac2f4c01d5249297b590",
     "grade": true,
     "grade_id": "cell-daca4a565756ec40",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make plots here\n",
    "# YOUR CODE HERE\n",
    "ensemble_classifier_handler.plot_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Part 3 [10 points]:** In the following space below discuss at least one advantage and disadvantage for *Random Forest* and *AdaBoost*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2593940b46e846297e4ec5f1bec3e768",
     "grade": true,
     "grade_id": "cell-15ec51992550c8fc",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**AdaBoost**\n",
    "\n",
    "Advantage: Very few parameters to tune, just the number of repetitions.\n",
    "\n",
    "Disadvantage: Sensitive to noisy data because of the use of weak classifiers.\n",
    "\n",
    "\n",
    "**Random Forest** \n",
    "\n",
    "Advantage: Combining the output of many small decision trees (bagging) results in less overfitting, which improves accuracy overall.\n",
    "\n",
    "Disadvantage: Generally slow due to the creation of many decision trees (or stumps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Optional survey.\n",
    "***\n",
    "\n",
    "We are always interested in your feedback. At the end of each homework, there is a simple anonymous feedback [survey](https://forms.gle/T9VKL7UPWjWoYxVu6) to solicit your feedback for how to improve the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
